{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4632de-7d07-4a45-9d63-58707a35cce3",
   "metadata": {},
   "source": [
    "## Bedrock embedding with cohere: Load testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac59f6f8-e517-49ba-bad0-030ce7814e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks lens:  20\n",
      "Chunks content first item:  We introduced Amazon Bedrock to the world a little over a year ago, delivering an entirely new way to build generative artificial intelligence (AI) applications. With the broadest selection of first- and third-party foundation models (FMs) as well as user-friendly capabilities, Amazon Bedrock is the fastest and easiest way to build and scale secure generative AI applications. Now tens of thousands of customers are using Amazon Bedrock to build and scale impressive applications. They are innovating quickly, easily, and securely to advance their AI strategies. And we’re supporting their efforts by enhancing Amazon Bedrock with exciting new capabilities including even more model choice and features that make it easier to select the right model, customize the model for a specific use case, and safeguard and scale generative AI applications.\n",
      "\n",
      "Customers across diverse industries from finance to travel and hospitality to healthcare to consumer technology are making remarkable progress. They are realizing real business value by quickly moving generative AI applications into production to improve customer experiences and increase operational efficiency. Consider the New York Stock Exchange (NYSE), the world’s largest capital market processing billions of transactions each day. NYSE is leveraging Amazon Bedrock’s choice of FMs and cutting-edge AI generative capabilities across several use cases, including the processing of thousands of pages of regulations to provide answers in easy-to-understand language\n",
      "\n",
      "\n",
      "\n",
      "Global airline United Airlines modernized their Passenger Service System to translate legacy passenger reservation codes into plain English so that agents can provide swift and efficient customer support. LexisNexis Legal & Professional, a leading global provider of information and analytics, developed a personalized legal generative AI assistant on Lexis+ AI. LexisNexis customers receive trusted results two times faster than the nearest competing product and can save up to five hours per week for legal research and su\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Boto3 client for Bedrock Runtime\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "def split_into_chunks(file_path, chunk_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "        return chunks\n",
    "\n",
    "# Example usage\n",
    "file_path = 'doc.txt'\n",
    "chunk_size = 2048  # Specify the desired chunk size\n",
    "\n",
    "chunks = split_into_chunks(file_path, chunk_size)\n",
    "print(\"Chunks lens: \", len(chunks))\n",
    "print(\"Chunks content first item: \", chunks[0])\n",
    "body = json.dumps({'texts': chunks, 'input_type': \"search_document\"})\n",
    "\n",
    "modelId = 'cohere.embed-multilingual-v3'\n",
    "accept = \"*/*\"\n",
    "contentType = 'application/json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cd881cd-6dcc-4668-ad35-1f2475ab55c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=modelId,\n",
    "    accept=accept,\n",
    "    contentType=contentType,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "response_body = json.loads(response.get('body').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc085be-240d-422e-a026-0a59134cf0c6",
   "metadata": {},
   "source": [
    "### Testing the throughput and lantency with locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac579ad7-1714-4be7-bd14-130a80939f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: locust in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.28.0)\n",
      "Requirement already satisfied: gevent>=22.10.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (23.9.0.post1)\n",
      "Requirement already satisfied: flask>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (3.0.2)\n",
      "Requirement already satisfied: Werkzeug>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (3.0.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (2.31.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (1.0.7)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (25.1.2)\n",
      "Requirement already satisfied: geventhttpclient>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (2.3.1)\n",
      "Requirement already satisfied: ConfigArgParse>=1.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (1.7)\n",
      "Requirement already satisfied: psutil>=5.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (5.9.8)\n",
      "Requirement already satisfied: Flask-Login>=0.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (0.6.3)\n",
      "Requirement already satisfied: Flask-Cors>=3.0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (4.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from locust) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from flask>=2.0.0->locust) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from flask>=2.0.0->locust) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from flask>=2.0.0->locust) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from flask>=2.0.0->locust) (1.7.0)\n",
      "Requirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gevent>=22.10.2->locust) (5.0)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gevent>=22.10.2->locust) (6.2)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gevent>=22.10.2->locust) (3.0.3)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geventhttpclient>=2.3.1->locust) (2024.2.2)\n",
      "Requirement already satisfied: brotli in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geventhttpclient>=2.3.1->locust) (1.1.0)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geventhttpclient>=2.3.1->locust) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->locust) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->locust) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Werkzeug>=2.0.0->locust) (2.1.5)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from zope.event->gevent>=22.10.2->locust) (69.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bdb3667-54b5-414e-8a24-6c9b240e3841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting locustfile.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile locustfile.py\n",
    "\n",
    "from locust import User, task, between\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Boto3 client for Bedrock Runtime\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "def split_into_chunks(file_path, chunk_size):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "        return chunks\n",
    "\n",
    "# Example usage\n",
    "file_path = 'doc.txt'\n",
    "chunk_size = 2048  # Specify the desired chunk size\n",
    "\n",
    "chunks = split_into_chunks(file_path, chunk_size)\n",
    "print(\"Chunks lens: \", len(chunks))\n",
    "print(\"Chunks content first item: \", chunks[0])\n",
    "body = json.dumps({'texts': chunks, 'input_type': \"search_document\"})\n",
    "\n",
    "modelId = 'cohere.embed-multilingual-v3'\n",
    "accept = \"*/*\"\n",
    "contentType = 'application/json'\n",
    "\n",
    "class LLMUser(User):\n",
    "    @task\n",
    "    def generation(self):\n",
    "        # Invoke the model\n",
    "        with self.environment.events.request.measure(\"[Send]\", \"Prompt\"):\n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=modelId,\n",
    "                accept=accept,\n",
    "                contentType=contentType,\n",
    "                body=body\n",
    "            )\n",
    "            # Process the response\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            \n",
    "        logging.info(\"Finished generation!\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe08be-e37e-4e59-9036-44e649954fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "875f7987-18bc-4deb-98b3-71a4e712c983",
   "metadata": {},
   "source": [
    "The configuration with Command Line Options https://docs.locust.io/en/stable/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db38d4-b323-463f-836d-51905c0e46a1",
   "metadata": {},
   "source": [
    "--users <int> Peak number of concurrent Locust users. Primarily used together with --headless or --autostart.\n",
    "    \n",
    "--headless Disable the web interface, and start the test immediately.\n",
    "    \n",
    "--csv Store request stats to files in CSV format.\n",
    "\n",
    "--spawn-rate <float> Rate to spawn users at (users per second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92b060-de61-4d6f-a63f-8225449ba41f",
   "metadata": {},
   "source": [
    "In this example, the --users option sets the total number of users to 30, and the --spawn-rate option sets the rate of user spawning to 30 users per second. By using the same value for --spawn-rate as the total number of users, all 30 users will be spawned immediately. Therefore, at any given time during the test, there will be a maximum of 30 concurrent users.\n",
    "\n",
    "Please note that the --run-time option sets the duration of the test in seconds. In this example, the test will run for 120 seconds before stopping.\n",
    "\n",
    "!locust --headless --users 10 --spawn-rate 10 --run-time 120 --csv ./benchmark_metric/benchmark_u30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa215728-03ec-4a37-85b8-2d7a625eeb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks lens:  20\n",
      "Chunks content first item:  We introduced Amazon Bedrock to the world a little over a year ago, delivering an entirely new way to build generative artificial intelligence (AI) applications. With the broadest selection of first- and third-party foundation models (FMs) as well as user-friendly capabilities, Amazon Bedrock is the fastest and easiest way to build and scale secure generative AI applications. Now tens of thousands of customers are using Amazon Bedrock to build and scale impressive applications. They are innovating quickly, easily, and securely to advance their AI strategies. And we’re supporting their efforts by enhancing Amazon Bedrock with exciting new capabilities including even more model choice and features that make it easier to select the right model, customize the model for a specific use case, and safeguard and scale generative AI applications.\n",
      "\n",
      "Customers across diverse industries from finance to travel and hospitality to healthcare to consumer technology are making remarkable progress. They are realizing real business value by quickly moving generative AI applications into production to improve customer experiences and increase operational efficiency. Consider the New York Stock Exchange (NYSE), the world’s largest capital market processing billions of transactions each day. NYSE is leveraging Amazon Bedrock’s choice of FMs and cutting-edge AI generative capabilities across several use cases, including the processing of thousands of pages of regulations to provide answers in easy-to-understand language\n",
      "\n",
      "\n",
      "\n",
      "Global airline United Airlines modernized their Passenger Service System to translate legacy passenger reservation codes into plain English so that agents can provide swift and efficient customer support. LexisNexis Legal & Professional, a leading global provider of information and analytics, developed a personalized legal generative AI assistant on Lexis+ AI. LexisNexis customers receive trusted results two times faster than the nearest competing product and can save up to five hours per week for legal research and su\n",
      "[2024-05-23 09:07:12,774] ip-172-16-168-10/INFO/locust.main: Run time limit set to 60 seconds\n",
      "[2024-05-23 09:07:12,774] ip-172-16-168-10/INFO/locust.main: Starting Locust 2.28.0\n",
      "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated       0     0(0.00%) |      0       0       0      0 |    0.00        0.00\n",
      "\n",
      "[2024-05-23 09:07:12,776] ip-172-16-168-10/INFO/locust.runners: Ramping to 10 users at a rate of 10.00 per second\n",
      "[2024-05-23 09:07:12,776] ip-172-16-168-10/INFO/locust.runners: All users spawned: {\"LLMUser\": 10} (10 total users)\n",
      "[2024-05-23 09:07:13,210] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,226] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,233] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,240] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,279] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,285] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,293] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,343] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,349] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,354] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,542] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,574] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,581] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,620] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,634] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,657] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,688] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,694] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,718] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,769] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,866] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,916] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:13,922] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:14,064] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:14,069] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "[2024-05-23 09:07:14,091] ip-172-16-168-10/INFO/root: Finished generation!\n",
      "KeyboardInterrupt\n",
      "2024-05-23T09:07:14Z\n",
      "[2024-05-23 09:07:14,110] ip-172-16-168-10/INFO/locust.main: Shutting down (exit code 0)\n",
      "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "[Send]   Prompt      38     0(0.00%) |    360      61     575    350 |   27.58        0.00\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated      38     0(0.00%) |    360      61     575    350 |   27.58        0.00\n",
      "\n",
      "Response time percentiles (approximated)\n",
      "Type     Name      50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
      "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "[Send]   Prompt      370    440    460    460    530    570    580    580    580    580    580     38\n",
      "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "         Aggregated      370    440    460    460    530    570    580    580    580    580    580     38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!locust --headless --users 10 --spawn-rate 10 --run-time 60 --csv ./benchmark_metric/benchmark_u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be37908-7c39-4756-bf28-0140d5ffa201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c7338-1358-4759-8577-fa26b14cad32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
